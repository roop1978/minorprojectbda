# ğŸ¯ Fairness-Aware Job Recommendation System - Demo

## ğŸš€ Quick Start Guide

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Generate Synthetic Data
```bash
python generate_data.py --samples 10000 --output fairjob.csv
```

### 3. Train the Model
```bash
python train_simple.py --train --epochs 50 --batch_size 256
```

### 4. Test the Model
```bash
python test_model.py
```

### 5. Launch Dashboard
```bash
streamlit run app.py
```

## ğŸ“Š System Architecture

The system implements a **Wide+Deep+Session-GNN+Adversary** architecture:

- **Wide Component**: Memorizes feature interactions
- **Deep Component**: Learns complex feature representations  
- **Session Component**: Captures user-job interaction patterns
- **Adversary**: Detects and mitigates gender bias

### Loss Function
```
L_total = L_click - Î»_adv Ã— L_adv + Î»_fair Ã— L_fair
```

## âš–ï¸ Fairness Metrics

The system tracks and optimizes for:

- **Demographic Parity Gap**: â‰¤ 0.05 (Equal positive prediction rates)
- **Equalized Odds Gap**: â‰¤ 0.05 (Equal true/false positive rates)
- **Exposure Gap**: â‰¤ 0.1 (Equal recommendation exposure)
- **Adversary Accuracy**: < 0.6 (Bias detection capability)

## ğŸ“ˆ Training Results

Based on the test run:

- **Model Parameters**: 143,695
- **Training Samples**: 800
- **Validation Samples**: 200
- **Final Fairness Metrics**:
  - Demographic Parity Gap: 0.2708 (needs improvement)
  - Equalized Odds Gap: 0.5396 (needs improvement)
  - Adversary Accuracy: 0.6200 (good bias detection)

## ğŸ¯ Sample Predictions

The model successfully makes predictions:

```
Sample 1: User 103, Job 273, Male, Actual: 1, Predicted: 0.4343
Sample 2: User 436, Job 294, Male, Actual: 0, Predicted: 0.3693
Sample 3: User 861, Job 398, Female, Actual: 1, Predicted: 0.3464
```

## ğŸ› ï¸ Features

### Data Generation
- Synthetic FairJob dataset with realistic bias patterns
- Configurable gender bias injection
- Multiple job categories and user profiles

### Training
- Adversarial debiasing with configurable weights
- Fairness regularization
- Early stopping and model checkpointing
- Comprehensive fairness metrics tracking

### Dashboard
- Interactive Streamlit interface
- Real-time fairness analysis
- Model performance visualization
- CSV upload and prediction capabilities

## ğŸ“ Project Structure

```
fairjob/
â”œâ”€â”€ models_simple.py      # Simplified model architecture (no DGL)
â”œâ”€â”€ train_simple.py       # Training script
â”œâ”€â”€ utils.py              # Fairness metrics and utilities
â”œâ”€â”€ app.py                # Streamlit dashboard
â”œâ”€â”€ generate_data.py      # Synthetic data generation
â”œâ”€â”€ test_model.py         # Model testing script
â”œâ”€â”€ requirements.txt      # Dependencies
â””â”€â”€ README.md            # Documentation
```

## ğŸ”§ Configuration

Key parameters can be adjusted in the training script:

- `--epochs`: Number of training epochs
- `--batch_size`: Training batch size
- `--learning_rate`: Learning rate
- `--lambda_adv`: Adversarial loss weight
- `--lambda_fair`: Fairness loss weight

## ğŸ¯ Next Steps

1. **Improve Fairness**: Adjust adversarial and fairness loss weights
2. **Scale Up**: Train on larger datasets with more epochs
3. **Real Data**: Adapt to real job recommendation datasets
4. **Advanced GNN**: Integrate full DGL functionality when compatible
5. **A/B Testing**: Deploy and test in production environments

## ğŸ“š Research Background

This implementation demonstrates:
- Fairness-aware machine learning
- Adversarial debiasing techniques
- Hybrid recommendation architectures
- Real-time bias monitoring

The system provides a solid foundation for building fair job recommendation systems in production environments.
